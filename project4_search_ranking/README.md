# Project 4 - Search Ranking Evaluation

This project simulates a real-world search ranking qualityi evaluation to evaluate how well search results match user intent.

The dataset includes 20 search queries, 200 ranked results, titles and descriptions that were generated by ChatGPT, rationales, relevance scoring (0-3), and the safety evaluation for high risk queries.

## 1. Files in this folder

### **'project4_search_ranking_dataset.csv'**
The dataset has:
- query_id
- query
- result_title
- description
- relevance_score
- rationale

## 2. Evaluation Objectives

This project assesses accurate interpretation of navigational, informational, transactional, local user intent, and safety-sensitive health queries using relevance scoring to ensure consistency and quality across all 200 results.

## 3. Scoring Rubric (0-3)

3 = Highly relevant: fully satisfies user intent,
2 = Moderately relevant: partially satisfies user intent (broader),
1 = Slightly relevant: Weak relevance but helpful (adjacent topics),
0 = Irrelevant: Not useful or dangerous (unsafe)

## 4. Qeury types 

### **Batch 1 - Navigational queries (1~4)
Direct actions such as app download, official support hotline, login portal, and dance tutorial navigation

### **Batch 2 - Informational queries (5~10)
How-to or educational content such as anxiety relief, pandas tutorial, intermittent fasting safety, and sleep improvement

### **Batch 3 - Transactional queries (11~14)
Purchasing intent or comparison such as smpartphone budget, workout routines, hair salon recommendations, and tips for cheap flight 

### **Batch 4 - Local intent (15~17)
Geographically anchored such as quiet study place, Korean BBQ, and nightlife in Korea

### **Batch 5 - Safety-sensitive queries (18~20)
High-risk queries that require safe ranking decisions to potentially downrank.

## 5. Methodology

- Step 1: Understand user intent behind the query.
- Step 2: Review the top 10 search results based on the title, description, and the content type.
- Step 3: Assgin proper relevance scores using 0-3 rubric.
- Step 4: Write rationale to justify each score ensuring consistency.

## 6. Key insights

- Insight 1: Navigational queries specifically demand precision. Official links should always be on the top rank.
- Insight 2: Recency highly affects relevance score gaining user trust. For example, 2025 guide tend to be more relevant than the general tutorial.
- Insight 3: Transactional queries often suffer from engagement bias. Highly engaging videos may overrank informative comparison videos.
- Insight 4: Local intent requires location-specifc sources prioritizing curated local lists, not general videos.
- Insight 5: Safety-sensitive queries must lower the rank of harmful contents.

## 7. Common ranking errors

- Error 1: Overranking irrelevant results that went viral over useful sources.
- Error 2: Underranking experts who can provide useful contents.
- Error 3: Midranking unsafe contents.

## 8. Summary 

This project demonstrates strong ability to evaluate search relevance by understanding various intents of search behaviours. Safety-first approach was used for sensitive queries while remaining consistent scoring and reasoning. 
